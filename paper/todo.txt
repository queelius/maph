


suppose we construct "perfect" hash filters with a closed hash function from S to Int.
in a perfect hash function, no collision from S occurs. however, we may relax this
and instead allow k collisions, such that if a collision occurs, use some open addressing
scheme like linear or quadratic probing or double hashing, and try k-1 more times. repeat.
if any element in S collides more than k times, another trial is triggered.

perf hashing is then special case k=0 and vareps = 2^(-k) where k is bit length of filter.

for k=1
vareps = 2 alpha (1 - alpha)
alpha = 1 - sqrt(1 - varepsilon)

----

Talk about how to implement the perfect hash filter and other
such data structures as RegularType (concept) through representational equality.
This also works for the set-theoretic stuff like unions, power set, etc.
by reducing to a canonical form and checking that the expression trees are
equivalent and each hash(object) is the same. note that this requires a determinstic
algorithm, which may not be ideal, so that the data constructor when applied to an
objective set maps to the same approx set representation always.

note: if show an encrypted search app, then an efficient expression tree may be used
and in the end traverse the document collection and insert them into a flat iterable
set and just do a simple equality comparison.


Increasing entropy:

    (1) make load factor a r.v. as described elsewhere.
    (2) try to make it so that the integer hashes chosen are rv's
        that approximate a uniform distribution, rather than a geoemetric. This
        sacrifices space efficiency for entropy / confidentiality.
    (3) given N entries in the matrix, i.e., a max hash N (elements in
        the set may have a max hash n < N, but N is still used for the
        modulus), the load factor may be estimated, assuming the adversary
        knows the algorithm used to generate the perfect hash function. thus,
        we may need to choose integer hashes that are distributed over a
        larger range than would be necessary in the optimal algorithm, i.e.,
        by (2) they are uniformly distributed and thus a normal binary code may
        be used, and by (3) we want the hashes to be r.v.'s distributed over a
        larger range, so that the code for each is (when not compressed) larger.
        However, (2) is the main thing.t
        
