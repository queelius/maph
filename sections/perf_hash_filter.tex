\documentclass[ ../main.tex]{subfiles}
\providecommand{\mainx}{..}
\begin{document}
\section{Implementation of approximate sets}
\label{sec:phf}
We consider the special case of positive approximate sets in which the universe of elements is given by the \emph{countably infinite} set of all bit strings.
\begin{definition}
The countably infinite set of all bit strings is denoted by
\begin{equation}
    \cisb = \left\{b \colon b \in \left\{0,1\right\}^*\right\}\,,
\end{equation}
where $*$ denotes all non-negative integers.
\end{definition}
A \emph{finite} subset of $\cisb$ is given by the following definition.
\begin{definition}
The \emph{finite set} of all bit strings of length $n$ is denoted by
\begin{equation}
    \cisb_n = \left\{b \colon b \in \left\{0,1\right\}^n\right\}
\end{equation}
with a cardinality given by
\begin{equation}
    \card{\cisb_n} = 2^n\,.
\end{equation}
\end{definition}

The \emph{Bloom filter}\cite{bf} is a well-known data structure that implements the \emph{positive approximate set} over $\cisb$ and obtains a space complexity given approximately by
\begin{equation}
    -1.44 \log_2 \varepsilon \; \si{bits \per element}\,.
\end{equation}
We consider an alternative data structure, denoted the \gls{gls-phf}, which also implements the \emph{positive approximate set} over $\cisb$ and compares favorably to the Bloom filter in many ways. In what follows.

A hash function is related to countable sets $\cisb$ and $\cisb_n$ and is given by the following definition.
\begin{definition}
A \emph{hash function} $\hash \colon \cisb \mapsto \cisb_n$ is a function such that all bit strings of arbitrary-length are mapped (hashed) to bit strings of fixed-length $n$.
\end{definition}
For a given $x \in \cisb$, $y = \hash(x) \in \cisb_n$ is denoted the \emph{hash} of $x$. The \gls{gls-phf} depends on a \gls{gls-ph} as given by the following definition.
\begin{definition}
A perfect hash function of the set $\St$, denoted by
\begin{equation}
    \ph_{\St} \colon \cisb \mapsto \mathbb{Z}^{+}\,,
\end{equation}
is a hash function such that the mapping from $\St$ to $\mathbb{Z}^+$ is \emph{one-to-one}.
\end{definition}
The \emph{load factor} of $\ph_{\St}$ is given by the following definition.
\begin{definition}
\label{def:loadfactor}
The load factor of $\ph_{\St}$ is a rational number given by
\begin{equation}
\label{eq:loadfactor}
    r = \frac{m}{N}\,,
\end{equation}
where $m = \card{\St}$, $N \geq m$, and $N$ is the maximum hash of $\ph_{\St}$.
\end{definition}
\begin{definition}
A \emph{minimal} perfect hash function has a load factor $1$, i.e., the mapping from $\St$ to $\mathbb{Z}^+$ is \emph{one-to-one} and \emph{onto}.
\end{definition}

The \gls{gls-phf} is a data structure given by the following definition.
\begin{definition}
The \gls{gls-phf} is a data structure that implements\footnote{In \cref{thm:fp_rate}, we prove the \gls{gls-phf} implements the static approximate set.} the abstract data type of the \emph{positive approximate set} where the universe of elements is given by $\cisb$.
\end{definition}
The \emph{minimal} perfect hash filter given by the following definition.
\begin{definition}
The \emph{minimal} perfect hash filter is a perfect hash filter with a load factor $1$, i.e., every element $x$ in the set being approximated is \emph{perfectly hashed}.
\end{definition}
Replacing the \emph{perfect hash function} with a \emph{$k$-perfect hash function}, where disjoint subsets of up to $k$ elements may collide, generalizes the \gls{gls-phf} into an approximate set where both \emph{false positives} and \emph{false negatives} are possible.\footnote{We may also simply approximate a subset of a set $\St$ to support non-zero false negative rates, but \emph{$k$-perfect hashing} generates a more efficient solution.}

\Cref{alg:PHF,alg:PHF_has} describe the implementations of \MakeApproxSet and \Contains respectively.
\begin{algorithm}[h]
    \caption{Implementation of \protect\MakeApproxSet}
    \label{alg:PHF}
    \SetKwProg{func}{function}{}{}
    \KwIn
    {
        \begin{itemize}
            \item[$\St$] The \gls{gls-finiteset} to approximate.
            \item[$\varepsilon$] The \gls{gls-fprate}.
            \item[$r$] The \gls{gls-loadfactor}.
        \end{itemize}
    }
    \KwOut
    {
        A \gls{gls-phf} that approximates $\St$ with a \gls{gls-fprate} $\varepsilon$ and a \gls{gls-loadfactor} $r$.
    }
    \func{\MakeApproxSet{$\St, \varepsilon \given r$}}
    {
        $\ph_{\St} \gets$ \PHFgen{$\St$, $r$}\;
        $N \gets \left\lceil \card{\St} / r \right\rceil$\;
        $M \gets \left\lceil -\log_2\!\left(\varepsilon\right) \right\rceil$\;
        $\matrx{V} \gets N$-by-$M$ bit matrix\;
        \For{$x \in \St$}
        {
            $i \gets \ph_{\St}\!\left(x\right)$\;
            $h \gets$ \ro{$x$}$\mod M$\;
            $\matrx{V}\!\left[i\right] \gets h$\;
        }
        \tcp{This tuple is sufficient to code the \gls{gls-phf}.}
        \Return $\left(\matrx{V}, \ph_{\St}\right)$\;
    }
\end{algorithm}
\begin{algorithm}[h]
    \caption{Implementation of \protect\Contains}
    \label{alg:PHF_has}
    \SetKwProg{func}{function}{}{}
    \KwIn
    {
        \begin{enumerate}
            \item[$\Sp$] The \gls{gls-phf} data structure that approximates $\St$ with a false positive rate $\varepsilon$ and a load factor $r$.
            \item[$x$] The bit string to test membership of in $\Sp$.
        \end{enumerate}
    }
    \KwOut
    {
        $\True$ if $x \in \St$, otherwise $\True$ with probability $\varepsilon$ and false with probability $1 - \varepsilon$.
    }
    \func{\Contains{$\Sp, x$}}
    {
        $M \gets$ \Columns{$\matrx{V}$}\;
        $N \gets$ \Rows{$\matrx{V}$}\;
        $i \gets \ph_{\St}\!\left(x\right)$\;
        $h \gets$ \ro{$x$}$ \mod N$\;
        \Return{$\matrx{V}\!\left[i\right] = h$}\;
    }
\end{algorithm}

\begin{example}
Consider a set $\St = \{x_1, x_2, x_3\}$ and suppose we wish to approximate this set with a false positive rate $\varepsilon = 2^{-3}$. The perfect hash filter depicted by \cref{fig:PHFilter} is a representation of this \emph{frozen approximate set}.
\end{example}
\begin{figure}
\caption[Perfect hash filter]{A perfect hash filter implementing a static approximate set $\{x_1,x_2,x_3\}$ with a false positive rate $\varepsilon = 2^{-3}$ and a load factor $r = \frac{3}{5}$.}
\label{fig:PHFilter}
\centering
\input{img/phfilter.tex}
\end{figure}

We make the following set of assumptions. See \cite{oph} for more on these assumptions.
\begin{assumption}
The set $\St$ consists of $m$ elements, where each element is drawn uniformly at random without replacement from $\cisb$.
\end{assumption}
\begin{assumption}
The perfect hash function $\ph_{\St}$ approximates a random oracle.
\end{assumption}
\begin{assumption}
\label{asm:ro}
The hash function $\ro \colon \cisb \mapsto \cisb_n$ approximates a random oracle.
\end{assumption}

The \gls{gls-phf} is an implementation of a \emph{static approximate set} as given by the following theorem.
\begin{theorem}
\label{thm:fp_rate}
A \gls{gls-phf} is an implementation of \emph{static approximate set} with a false positive rate that is a rational number given by
\begin{equation}
\label{eq:fp_rate}
    \varepsilon \in \left\{ 2^{-k} \colon k \in \mathbb{Z}^+\right\} \subset \left(0, 1\right]\,.
\end{equation}
\end{theorem}
\begin{proof}
For the \gls{gls-phf} to be an implementation of approximate set $\Sp \supset \St$, by \cref{def:approx_set} the following conditions must be met:
\begin{enumerate}
\item\label{itm:fp_rate_1} For all $x \in \St$, \Contains{$\Sp$,$x$} returns $\True$.
\item\label{itm:fp_rate_2} For any $x \notin \St$, \Contains{$\Sp$,$x$} returns $\True$ with probability $\varepsilon$ and returns $\False$ with probability $1 - \varepsilon$.
\end{enumerate}

First, we prove condition \ref{itm:fp_rate_1}. For some $x \in \cisb$,
let $i = \ph_{\St}(x)$. For \Contains{$\Sp$,$x$} to return \True, row $\matrx{V}[i]$ must equal the hash given by \hash{$x$}. Since \ph$_{\St}$ perfectly hashes all $y \in \St$ and $x \in \St$, no other member of $\St$ hashes ?, and $\matrx{V}[i]$ is explicitly set to the hash $\hash(x)$ during the construction of the \gls{gls-phf} as given by \cref{alg:PHF}.

Second, we prove condition \ref{itm:fp_rate_2}. Observe that $\ph_{\St}$ is a hash function that takes any $x \in \cisb$ and maps it to an integer between $1$ and $N$. The corresponding row of $\matrx{V}$ consists of $M$ bits. There are $2^M$ possible bit strings of length $M$ and, by \cref{asm:ro}, \hash uniformly distributes over these possibilities. Thus, for $x \notin \St$, the probability that \hash{$x$} is equal to the corresponding $M$ bits is given by the false positive rate
\begin{equation}
\varepsilon = 2^{-M}\,.
\end{equation}
Since $M$ is any non-negative integer, the false positive rate is given by \cref{eq:fp_rate}.
\end{proof}

\subsection{Space complexity}
The minimum possible bit length of a data structure implementing the abstract data type of the \emph{approximate set} is given by the following postulate.
\begin{postulate}
\label{pst:approx_l_b}
The information-theoretic lower-bound of a data structure that implements the \emph{approximate set} abstract data type has an \emph{expected} bit length given by
\begin{equation}
    -\log_2 \varepsilon \; \si{bits \per element}\,,
\end{equation}
where $\varepsilon$ is the false positive rate.
\end{postulate}

The \emph{expected} bit length of the \emph{minimal} \gls{gls-phf} has a lower-bound given by the following theorem.
\begin{theorem}
\label{thm:mPHF_size}
The \emph{expected} bit length of the \gls{gls-phf} with a load factor $r$ has a lower-bound given by
\begin{equation}
\label{eq:mPHF_size}
    \log_2 e - \frac{1}{r} \log_2 \varepsilon - \left(\frac{1}{r} - 1\right) \log_2\left(\frac{1}{1-r}\right)\; \si{bits \per element}\,,
\end{equation}
where $\varepsilon$ is the false positive rate and $r$ is the load factor.
\end{theorem}
\begin{proof}
The proof comes in two parts. First, according to Theorem~3 in \cite{oph}, the \emph{expected} lower-bound for a perfect hash function with a load factor $r$ is given by
\begin{equation}
    \log_2 \mathrm{e} - \left(\frac{1}{r} - 1\right) \log_2\!\left(\frac{1}{1-r}\right) \si{bits \per element}\,.
\end{equation}
Second, a perfect hash function with a load factor $r$ maps each $x \in \St$ to a unique integer between $1$ and $\frac{m}{r}$, where $m=\card{\St}$, which requires a bit matrix $\matrx{V}$ with $m$ rows. Also, as shown in the proof for \cref{thm:fp_rate}, a false positive rate $\varepsilon$ requires $M = -\log_2 \varepsilon$ bits. Thus, the bit matrix $\matrx{V}$ has $m$ rows and $-\log_2 \varepsilon$ columns, resulting in a total of
\begin{equation}
    -\frac{m}{r} \log_2 \varepsilon \; \si{bits}\,.
\end{equation}
Adding both parts of the proof together results in
\begin{equation}
    -\frac{m}{r} \log_2 \varepsilon + m \log_2 \mathrm{e} - m \left(\frac{1}{r} - 1\right) \log_2\!\left(\frac{1}{1-r}\right) \si{bits}\,.
\end{equation}
Since we are interested in the bits per element, we divide by $m$ yielding the result
\begin{equation}
    -\frac{1}{r} \log_2 \varepsilon + \log_2 \mathrm{e} -  \left(\frac{1}{r} - 1\right) \log_2\!\left(\frac{1}{1-r}\right) \si{bits \per element}\,.
\end{equation}
\end{proof}

\begin{corollary}
The \emph{expected} bit length of the \emph{minimal} \gls{gls-phf} has a lower-bound given by
\begin{equation}
\label{eq:mPHF_size}
    \log_2 \frac{e}{\varepsilon} \; \si{bits \per element}\,,
\end{equation}
where $\varepsilon$ is the false positive rate.
\end{corollary}
\begin{proof}
First, $1.44 \si{bits}$ is the \emph{expected} bits per element for optimally space-efficient \emph{minimal perfect hash functions}.

Second, a \emph{minimal} perfect hash function maps each $x \in \St$ to a unique integer between $1$ and $m=\card{\St}$ which requires a bit matrix $\matrx{V}$ with $m$ rows. Also, as shown in the proof for \cref{thm:fp_rate}, a false positive rate $\varepsilon$ requires $M = -\log_2 \varepsilon$ bits. Thus, the bit matrix $\matrx{V}$ has $m$ rows and $-\log_2 \varepsilon$ columns, resulting in a total of
\begin{equation}
    -m \log_2 \varepsilon \; \si{bits}\,.
\end{equation}
Adding both parts of the proof together results in
\begin{equation}
    -m \log_2 \varepsilon + 1.44 m\; \si{bits}\,.
\end{equation}
Since we are interested in the bits per element, we divide by $m$ yielding the result
\begin{equation}
    1.44 - \log_2 \varepsilon\; \si{bits \per element}\,.
\end{equation}
\end{proof}


\begin{figure}
\centering
\caption{Optimal load factor $r^*$ for a given false positive rates $\varepsilon$ for \gls{gls-phf} $X$. We compute the corresponding absolute efficiency and the relative efficiency to \gls{gls-blf} $Y$.}
\label{fig:abs_eff}
\begin{tabular}{c *{3}{S[table-auto-round,
                 table-format=-1.3]}}
{$\varepsilon$} & {$r^*$} & {$e^*(X)$} & {$e(X,Y)$}\\
\hline
$2^{-1}$  & 0.768039 & 0.474373 & 0.684375\\ 
$2^{-2}$  & 0.898172 & 0.606835 & 0.875478\\
$2^{-3}$  & 0.951741 & 0.686016 & 0.989713\\
$2^{-4}$  & 0.97646 & 0.739545 & 1.06694\\
$2^{-5}$  & 0.988369 & 0.778099 & 1.12256\\
$2^{-6}$  & 0.994219 & 0.807064 & 1.16435\\
$2^{-7}$  & 0.997118 & 0.829528 & 1.19676\\
$2^{-8}$  & 0.998561 & 0.847402 & 1.22254\\
$2^{-9}$  & 0.999281 & 0.861932 & 1.24351\\
$2^{-10}$ & 0.999641 & 0.87396 & 1.26086\\
$2^{-20}$ & 1 & 0.932719 & 1.34563\\
$2^{-30}$ & 1 & 0.954117 & 1.3765\\
$2^{-60}$ & 1 & 0.97652 & 1.40882\\
$2^{-120}$ & 1 & 0.98812 & 1.42556\\
\end{tabular}
\end{figure}

\begin{figure}
\caption{Relative efficiency of the Perfect Hash Filter $X$ to the Bloom filter $Y$}
\label{fig:rel_eff_bf_PHF}
\centering
\begin{tikzpicture}
\begin{axis}[
    log basis x=2,
    xmode=log,
    ymin=.684,
    xmin=0.00097,
    xmax=.75,
    xlabel={false positive rate $\varepsilon$},
    ylabel={relative efficiency $e(X,Y)$},
    legend pos=north west,
    ]
    \addplot[dashed] coordinates {(0.125,0) (0.125,1.15)};
    \addplot[dotted,mark=square*] table
    {
        7.5231638e-37   1.42556
        8.6736174e-19   1.40882
        9.3132257e-10   1.3765
        9.53674316e-7   1.34563
        0.000976563     1.26086
        0.00195313      1.24351
        0.00390625      1.22254
        0.0078125       1.19676
        0.015625        1.16435
        0.03125         1.12256
        0.0625          1.06694
        0.125           0.989713
        0.25            0.875478
        0.5             0.684375
    };
    \node[label={$e(X,Y) \approx 1$ at}] at (axis cs:0.125,1.175) {};
    \node[label={$\varepsilon = 0.125$}] at (axis cs:0.125,1.1375) {};
  \end{axis}
\end{tikzpicture}
\end{figure}

The bit length of an object $x$ is denoted by
\begin{equation}
    \BL(x)\,,
\end{equation}
e.g., the bit length of any $x \in \cisb_n$ is given by $\BL(x) = n$. The relative space efficiency metric is given by the following definition.
\begin{definition}
The relative space efficiency of a data structure $X$ to a data structure $Y$ is given by the ratio of the bit length of $Y$ to the bit length of $X$ as given by
\begin{equation}
    e(X,Y)=\frac{\BL(Y)}{\BL(X)}\,,
\end{equation}
where $\BL$ is the bit length function. There are three possibilities:
\begin{enumerate}
    \item $\RE(X,Y) < 1 \iff X$ is less efficient than $Y$.
    \item $\RE(X,Y) = 1 \iff X$ and $Y$ are equally efficient.
    \item $\RE(X,Y) > 1 \iff X$ is more efficient than $Y$.
\end{enumerate}
\end{definition}

The \emph{absolute} space efficiency of an approximate set $X$ with a false positive rate $\varepsilon$ is given by
\begin{equation}
    \AE\!(X) = e(X,-\log_2 \varepsilon)\,.
\end{equation}
An asymptotically optimal space-efficient data structure has an absolute efficiency that goes to $1$ as the cardinality $m$ of the set being approximated goes to $\infty$. The absolute efficiency of the \gls{gls-phf} is not asymptotically optimal since it has an asymptotic limit less than $1$ that is a function of the false positive rate $\varepsilon$ as shown in \cref{fig:abs_eff}. However, as $\varepsilon$ goes to $0$ the absolute efficiency goes to $1$.

\paragraph{Comparisons to the \emph{\gls{gls-blf}}.}
The \gls{gls-phf} is most suitable as an immutable data structure. It is optimized for read-many, write-once usage patterns. Once it is constructed, only on the order of
\begin{equation}
    \mathcal{O} \left( 1 \right)
\end{equation}
hash functions need to be computed to test an element for membership. The Bloom filter is suitable as a mutalbe data structure; elements may be inserted into it at any point at a cost of increasing the false positive rates. However, it requires computing on the order of
\begin{equation}
    \mathcal{O}\left(-\log \varepsilon\right)
\end{equation}
hash functions per membership test.

The \emph{Bloom filter} has an expected encoding size given approximately by
\begin{equation}
    -1.44 \log_2 \varepsilon \; \si{bits \per element}\,.
\end{equation}
The absolute efficiency of the \gls{gls-blf} is approximately $\ln 2 \approx 0.69$.

The relative efficiency of the \gls{gls-phf} to the \gls{gls-blf} is a function of the false positive rate $\varepsilon$ as shown in \cref{fig:rel_eff_bf_PHF,fig:abs_eff}. If $\varepsilon < 2^{-3}$, the \gls{gls-blf} is more space efficient; otherwise the \gls{gls-phf} is more space efficient. As $\varepsilon$ goes to $0$, the relative efficiency of the \gls{gls-phf} to the \gls{gls-blf} goes to $1.44$. 

\subsection{Entropy}
If the load factor is known, the cardinality of $\St \subset \Sp$ is given by
\begin{equation}
    \card{\St} = N r\,.
\end{equation}

If the load factor is not known, then the cardinality may be probabilistically estimated.
According to \cite{}, the probability mass of the perfect hash function is given by
\begin{equation}
    \rv{N} \sim \pmf_{\rv{N}}(n \given m, r) = q^{2^n-1}\left(1 - q^{2^n}\right)
\end{equation}
where $q = 1 - \frac{(m/r)!}{(m/r)^m ((m/r)-m)!}$, $m = \card{\St}$, and $r$ is the load factor. Since $m = N r$

It does contain some information. We know that if the size of the vector of hashes is $m$, $\card{\St} \geq  $


\begin{theorem}
The random bit string that codes the \gls{gls-phf} is a \emph{maximum entropy} coder for the \emph{approximate set} abstract data type.
\end{theorem}
\begin{proof}
For a random bit $\rv{X}$ to have maximum entropy, it must be Bernoulli distribution with equiprobability, i.e.,
\begin{equation}
    \rv{X} \sim \berdist\!\left(p = \frac{1}{2}\right)\,.
\end{equation}

The \gls{gls-phf} of $\St$ is coded by two parts. The first party is a \emph{cryptographic} hash of $m$ elements in $\St$ concatenated with a bit string $b_n$, denoted by $h_k$. The \emph{cryptographic} hash function uniformly distributes over $\cisb_k$, and so $h_k$ has maximum entropy.

The second part is a bit string $b_n$ of bit length $n$.
\end{proof}

The only information contained in the code, prior to memmbership test, is given by the probality mass function of the random bit length $\rv{N}$, which is a function of the cardality of the \gls{gls-finiteset} it is approximating.

The random bit length $\rv{N}$ of the string has a probability mass concentrated around
\begin{equation}
    -m \log_2 \varepsilon + 1.44 m\,.
\end{equation}


\begin{theorem}
An estimator of the cardinality of an approximated set represented by a \gls{gls-phf} is given by
\begin{equation}
    \hat{m} = -\frac{\BL\!\left(\Sp\right)}{-\log_2 \varepsilon + 1.44}\,,
\end{equation}
were $\BL$ is the bit length function and $\varepsilon$ is the \emph{\gls{gls-fprate}}.
\end{theorem}
\begin{proof}
The \emph{expected} bit length is given by
\begin{equation*}
    \expectation[\rv{N}] = -m \log_2 \varepsilon + 1.44 m\,, 
    \tag{\ref{} revisited}
\end{equation*}
where $m$ is the cardinality of the approximated set. Thus, the maximumn likelihood estimator is given by assuming the bit length of the \gls{gls-phf} realizes this expectation,
\begin{equation}
    \BL(\Sp) = -\hat{m} \log_2 \varepsilon + 1.44 \hat{m}\,.
\end{equation}
Solving for $\hat{m}$ results in the estimator
\begin{equation}
    \hat{m} = \frac{\BL(\Sp)}{-\log_2 \varepsilon + 1.44}\,.
\end{equation}
\end{proof}



\begin{theorem}
A \emph{maximum likelihood estimator} of the cardinality of an approximated set represented by a \gls{gls-phf} is given by
\begin{equation}
    \hat{m} = -\frac{\BL\!\left(\Sp\right)}{-\log_2 \varepsilon + 1.44}\,,
\end{equation}
were $\BL$ is the bit length function and $\varepsilon$ is the \emph{\gls{gls-fprate}}.
\end{theorem}
\begin{proof}
The \emph{expected} bit length is given by
\begin{equation*}
    \expectation[\rv{N}] = -m \log_2 \varepsilon + 1.44 m\,, 
    \tag{\ref{} revisited}
\end{equation*}
where $m$ is the cardinality of the approximated set. Thus, the maximumn likelihood estimator is given by assuming the bit length of the \gls{gls-phf} realizes this expectation,
\begin{equation}
    \BL(\Sp) = -\hat{m} \log_2 \varepsilon + 1.44 \hat{m}\,.
\end{equation}
Solving for $\hat{m}$ results in the estimator
\begin{equation}
    \hat{m} = \frac{\BL(\Sp)}{-\log_2 \varepsilon + 1.44}\,.
\end{equation}
\end{proof}
However, there is actually a simpler method. Simply count the number of rows in the matrix $\matrx{V}$.

We made trade space complexity for entropy if desired. For instance, if we find a bit string of a length around $t > -m \log_2 \varepsilon$ that codes the \gls{gls-phf}, then an estimate of the cardinality is given by
\begin{equation}
    \hat{\card{\St}} = -\frac{t}{\log_2 \varepsilon}\,.
\end{equation}
Thus, without knowing how $t$ was chosen, we can only infer that the approximated set has a cardinality approximately between $0$ and $-\frac{t}{\log_2 \varepsilon}$, which has an approximate entropy of $\log_2 t$. The coding efficiency is now given by
\begin{equation}
    0 < \frac{m}{t} \leq 1\,,
\end{equation}
which is the \emph{maximum} efficiency possible for an approximate set with $\log_2 t$ entropy for the cardinality.

\subsection{C++ implementation}
\lstinputlisting
[
    float,
    language=c++,
    frame=lines,
    label={code:sas},
    caption={The C++ interface of the \emph{static approximate set}} abstract data type
]
{code/approximate_set.h}

\lstinputlisting
[
    float,
    language=c++,
    frame=lines,
    label={code:phf},
    caption={The C++ interface of the \gls{gls-phf}}
]
{code/perfect_hash_filter.h}


The theoretical \gls{gls-phf} based off of the random oracle as described by \cite{oph} is not tractable. Its purpose was to demonstrate theoretical properties.

We use a fast state-of-the-art algorithm \cite{chd} for generating perfect hash functions that requires $\alpha(r) \; \si{bits \per element}$, where $\alpha$ is a function of the \gls{gls-loadfactor} $r$ which does not obtain the \emph{expected} lower-bound. The expected bit length of this \gls{gls-phf} is given by
\begin{equation}
    -\frac{1}{r} \log_2 \varepsilon + \alpha(r) \; \si{bits \per element}
\end{equation}
and its absolute efficiency is given by
\begin{equation}
    \frac{-\frac{1}{r}\log_2 \varepsilon + \alpha(r)}{-\log_2 \varepsilon}
\end{equation}


The relative efficiency of the Bloom filter to the perfect hash filter is given by
\begin{equation}
     0.69 - \frac{\alpha(r)}{1.44 \log_2 \varepsilon}
\end{equation}
As $\varepsilon \to 0$, the relative efficiency goes to $0.69$. In the case of a the \emph{minimal} perfect hash filter,
\begin{equation}
    \alpha(1) = 2.07\,.
\end{equation}
The relative efficiency of the Bloom filter to the \emph{minimal} perfect hash filter is given by
\begin{equation}
    \ln 2 - \frac{1.4375}{\log_2 \varepsilon}\,.
\end{equation}
The point at which they are equally efficient efficient is $\varepsilon \approx 2^{-4}$. Thus, the \emph{minimal} perfect hash filter is more efficient if $\varepsilon < 2^{-4}$ and otherwise the Bloom filter is more efficient. See \cref{fig:rel_eff} for a visualization.
\begin{figure}
\caption{Relative efficiency of the Bloom filter to the \emph{minimal} perfect hash filter}
\label{fig:rel_eff}
\centering
\begin{tikzpicture}
\begin{axis}[
    log basis x=2,
    xmode=log,
    ymin=.69,
    xmin=0.0008,
    xmax=.75,
    xlabel={$\varepsilon$},
    ylabel={relative efficiency}
    ]
    \addplot[dotted,samples=1000,domain=0.0008:.5]{-1.4375/log2(x)+.69};
    \addplot[dashed] coordinates {(0.0401879,0) (0.0401879,1.5)};
    \node[label={break even}] at (axis cs:0.0401879,1.5) {};
    \node[circle,fill,inner sep=2pt] at (axis cs:0.000976563,0.83375) {};
    \node[circle,fill,inner sep=2pt] at (axis cs:0.00195313,0.849722) {};
    \node[circle,fill,inner sep=2pt] at (axis cs:0.00390625,0.869687) {};
    \node[circle,fill,inner sep=2pt] at (axis cs:0.0078125,0.895357) {};
    \node[circle,fill,inner sep=2pt] at (axis cs:0.015625,0.929583) {};
    \node[circle,fill,inner sep=2pt] at (axis cs:0.03125,0.9775) {};
    \node[circle,fill,inner sep=2pt] at (axis cs:0.0625,1.05) {};
    \node[circle,fill,inner sep=2pt] at (axis cs:0.125,1.16917) {};
    \node[circle,fill,inner sep=2pt] at (axis cs:0.25,1.40875) {};
    \node[circle,fill,inner sep=2pt] at (axis cs:0.5,2.1275) {};
  \end{axis}
\end{tikzpicture}
\end{figure}
\end{document}